{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<img src=\"images\\logo_datai.png\" width=\"400\" img style=\"float: right;\"> \n",
    "\n",
    "https://www.unav.edu/web/instituto-de-ciencia-de-los-datos-e-inteligencia-artificial<br>\n",
    "Author: Pablo Urruchi Mohino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import urllib.request as urllib2\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python - Beautifulsoup\n",
    "\n",
    "\n",
    "References: [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "When we visit a web page, our web browser makes a request to a web server. This request is called a GET request, since we're getting files from the server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:\n",
    "\n",
    "HTML — contain the main content of the page. <br>\n",
    "CSS — add styling to make the page look nicer. <br>\n",
    "JS — Javascript files add interactivity to web pages. <br>\n",
    "Images — image formats, such as JPG and PNG allow web pages to show pictures.\n",
    "\n",
    "After our browser receives all the files, it renders the page and displays it to us. There's a lot that happens behind the scenes to render a page nicely, but we don't need to worry about most of it when we're web scraping. When we perform web scraping, we're interested in the main content of the web page, so we look at the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML\n",
    "HyperText Markup Language (HTML) is a language that web pages are created in. HTML isn't a programming language, like Python. Instead, it's a markup language that tells a browser how to layout content. HTML allows you to do similar things to what you do in a word processor like Microsoft Word: make text bold, create paragraphs, and so on. Because HTML isn't a programming language, it isn't nearly as complex as Python.\n",
    "\n",
    "Right inside an html tag, we put two other tags, the head tag, and the body tag. The main content of the web page goes into the body tag. The head tag contains data about the title of the page, and other information that generally isn't useful in web scraping:\n",
    "\n",
    "<img src=\"images\\bsoup1.png\" width=\"500\" img style=\"float: center;\">\n",
    "\n",
    "You may have noticed above that we put the head and body tags inside the html tag. In HTML, tags are nested, and can go inside other tags.\n",
    "\n",
    "\n",
    "\n",
    "Tags have commonly used names that depend on their position in relation to other tags:\n",
    "\n",
    "child — a child is a tag inside another tag. So the two p tags above are both children of the body tag.\n",
    "parent — a parent is the tag another tag is inside. Above, the html tag is the parent of the body tag.\n",
    "sibiling — a sibiling is a tag that is nested inside the same parent as another tag. For example, head and body are siblings, since they're both inside html. Both p tags are siblings, since they're both inside body.\n",
    "\n",
    "\n",
    "Here are a few common html tags:\n",
    "\n",
    "p - paragraph <br>\n",
    "a - indicates a link <br>\n",
    "div - indicates a division, or area, of the page. <br>\n",
    "b - bolds any text inside. <br>\n",
    "i - italicizes any text inside. <br>\n",
    "table - creates a table. <br>\n",
    "form - creates an input form. <br>\n",
    "\n",
    "This is the complete list of elements you might find: <br>\n",
    "https://developer.mozilla.org/en-US/docs/Web/HTML/Element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: what indicates the h1 tag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests\n",
    "\n",
    "The requests library will make a GET request to a web server, which will download the HTML contents of a given web page for us. \n",
    "\n",
    "status_code indicates whether the download was successful or not.\n",
    "content has the html downloaded content.\n",
    "\n",
    "resources: <br>\n",
    "http://docs.python-requests.org/en/master/ <br>\n",
    "http://www.dataquest.io/blog/python-api-tutorial/ <br>\n",
    "http://docs.python-requests.org/en/master/user/quickstart/#response-content <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.example.com'\n",
    "#headers = {'User-Agent' : 'OpenGraph Extractor (YOUR_EMAIL_HERE)'}\n",
    "url = requests.get(url)#, None, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautifulsoup\n",
    "\n",
    "Beautifulsoup is essentialy a parser for html content. In fact, you can specify the parser you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(url.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_node = soup.find('meta', property = 'og:description')\n",
    "if description_node:\n",
    "    description = description_node.get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select all the elements at the top level of the page using the children property of soup. Note that children returns a list generator, so we need to call the list function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type of each element in the list:\n",
    "[type(item) for item in list(soup.children)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all of the items are BeautifulSoup objects. The first is a Doctype object, which contains information about the type of the document. The second is a NavigableString, which represents text found in the HTML document. The final item is a Tag object, which contains other nested tags. The most important object type, and the one we'll deal with most often, is the Tag object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now niceliy print the content with prettify, a beautifulsoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a single tag, we can simply use the find_all method, which will find all the instances of a tag on a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that find_all returns a list, so we'll have to loop through, or use list indexing, it to extract text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you instead only want to find the first instance of a tag, you can use the find method, which will return a single BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beautifulsoup - Searching for tags by class and id\n",
    "\n",
    "Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. We can also use them when scraping to specify specific elements we want to scrape. \n",
    "\n",
    "We can use the find_all method to search for items by class or by id. In the below example, we'll search for any p tag that has the class period-name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_='class_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying a tag, you can also search for any tag which mets your requirements as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_=\"class_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even search elements by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id=\"id_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS\n",
    "\n",
    "Another way of seraching items is through CSS selectors. These selectors are how the CSS language allows developers to specify HTML tags to style. Here are some examples:\n",
    "\n",
    "p a — finds all a tags inside of a p tag. <br>\n",
    "body p a — finds all a tags inside of a p tag inside of a body tag. <br>\n",
    "html body — finds all body tags inside of an html tag. <br>\n",
    "p.outer-text — finds all p tags with a class of outer-text.  <br>\n",
    "p#first — finds all p tags with an id of first. <br>\n",
    "body p.outer-text — finds any p tags with a class of outer-text inside of a body tag. <br>\n",
    "\n",
    "You can find a complete list of selectors here https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Getting_started/Selectors\n",
    "\n",
    "BeautifulSoup objects support searching a page via CSS selectors using the select method. We can use CSS selectors to find all the p tags in our page that are inside of a div like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Weather data\n",
    "\n",
    "We will now put this in practice: Within the \"seven_day\" object, you can select as well one of the specific elements within it. Identify them withe the name for this particular case as \"tombstone-container\".\n",
    "\n",
    "<img src=\"images\\inspecting4.png\" width=\"700\" img style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\")\n",
    "url = \"http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\"\n",
    "headers = {'User-Agent' : 'OpenGraph Extractor (YOUR_EMAIL_HERE)'}\n",
    "url = requests.get(url)#, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "description_node = soup.find('meta', property = 'og:description')\n",
    "if description_node:\n",
    "    description = description_node.get('content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the element of your interest. You will find the source code \"blocks\" are related to the html content. Load that content in an object.\n",
    "\n",
    "<img src=\"images\\inspecting1.png\" width=\"400\" img style=\"float: left;\">\n",
    "<img src=\"images\\inspecting3.png\" width=\"500\" img style=\"float: right;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day = soup.find(id=\"seven-day-forecast\")\n",
    "seven_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "forecast_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"seven_day\" is composed by all this sub elements in the block:\n",
    "seven_day.find_all('p', attrs={'class' : 'period-name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another way to doing it in a more elegant way\n",
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the first element \"tonight\"\n",
    "tonight = forecast_items[2]\n",
    "tonight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort properly with prettyfy\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the specific text for each class\n",
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "print(period)\n",
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "print(short_desc)\n",
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the specific text for each class in a loop for the whole \"seven_day\"'s object\n",
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "\n",
    "print(short_descs)\n",
    "print(temps)\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all your columns in a single data frame and give it headers\n",
    "weather = pd.DataFrame({\n",
    "        \"period\": periods, \n",
    "        \"short_desc\": short_descs, \n",
    "        \"temp\": temps, \n",
    "        \"desc\":descs\n",
    "    })\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use regular expressions to clean the temperature as an integer\n",
    "temp_nums = weather[\"temp\"].str.extract(\"(?P<temp_num>\\d+)\", expand=False)\n",
    "weather[\"temp_num\"] = temp_nums.astype('int')\n",
    "temp_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_night = weather[\"temp\"].str.contains(\"Low\")\n",
    "weather[\"is_night\"] = is_night\n",
    "is_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[is_night]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "__1. Get the current weather information__\n",
    "\n",
    "<img src=\"images\\inspecting5.png\" width=\"600\" img style=\"float: center;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Get the details of the current forecast__\n",
    "<img src=\"images\\inspecting6.png\" width=\"600\" img style=\"float: center;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Get satellite image__\n",
    "<img src=\"images\\satellite_png.png\" width=\"800\" img style=\"float: center;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some additional notes\n",
    "\n",
    "### Legal risks of web scraping\n",
    "As handy as web scraping is, it also comes with some legal risks. Since the website operator has actually intended for their website to be used by humans, automated data retrievals using web scrapers can constitute a violation of the terms of use. This is especially true when it is retrieving large amounts of data from multiple pages simultaneously or in rapid succession. A human could not interact with the website in this way.\n",
    "\n",
    "Furthermore, the automated retrieval, storage and analysis of the data published on the website may constitute a violation of copyright law. If the scraped data contains personally identifiable information, storing and analyzing it without the consent of the person concerned might violate current data protection regulations, e.g. GDPR or CCPA. For example, it is prohibited to scrape Facebook profiles to collect personal information.\n",
    "\n",
    "### Technical limitations of web scraping\n",
    "It is often in the interest of website operators to limit the automated scraping of their online offers. Firstly, if large numbers of scrapers access the website, this can negatively affect its performance. Secondly, there are often internal areas of a website that should not appear in search results.\n",
    "\n",
    "The robots.txt standard was established to limit scrapers’ access to websites. To do so, the website operator places a text file called robots.txt in the root directory of the website. In this file, there are specific entries that define which scrapers or bots are allowed to access which areas of the website. The entries in the robots.txt file always apply to the entire domain.\n",
    "\n",
    "The following is an example of a robots.txt file that disallows scraping by any bot across the entire website:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Any bot\n",
    "User-agent: *\n",
    "# Disallow for the entire root directory\n",
    "Disallow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adhering to the restrictions laid out in the robots.txt file is completely voluntary. The bots are supposed to comply with the specifications, but technically, this cannot be enforced. Therefore, to effectively regulate web scrapers’ access to their websites, website operators also use more aggressive techniques. These techniques include restricting their access by limiting throughput and blocking their IP addresses if they repeatedly access the site ignoring the specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs as an alternative to web scraping\n",
    "While web scraping can be useful, it is not the preferred method for obtaining data from websites. There is often a better way to get this done. Many website operators present their data in a structured, machine-readable format. This data is accessed via special programming interfaces called application programming interfaces (APIs).\n",
    "\n",
    "There are significant advantages to using an API:\n",
    "\n",
    "The API is explicitly made available by the provider for the purpose of accessing the data: There are fewer legal risks, and it is easier for the provider to control access to the data. For example, an API key may be required to access the data. The provider can also limit throughput more precisely.\n",
    "The API delivers the data directly in a machine-readable format: This eliminates the need to tediously extract the data from the source code. In addition, the data structure is separate from its graphical presentation. The structure therefore remains the same even if the website design is changed.\n",
    "If there is an API available that provides access to all the data, this is the preferred way to access it. However, scraping can in principle be used to retrieve all text presented in a human-readable format on web pages.\n",
    "\n",
    "### Appendix A: Web scraping advice\n",
    "\n",
    "- Web scraping works best with **static, well-structured web pages**. Dynamic or interactive content on a web page is often not accessible through the HTML source, which makes scraping it much harder!\n",
    "- Web scraping is a \"fragile\" approach for building a dataset. The HTML on a page you are scraping can **change at any time**, which may cause your scraper to stop working.\n",
    "- If you can **download the data** you need from a website, or if the website provides an **API with data access**, those approaches are preferable to scraping since they are easier to implement and less likely to break.\n",
    "- If you are **scraping a lot of pages** from the same website (in rapid succession), it's best to insert delays in your code so that you don't overwhelm the website with requests. If the website decides you are causing a problem, they can block your IP address (which may affect everyone in your building!)\n",
    "- Before scraping a website, you should review its **robots.txt file** (also known as the [Robots exclusion standard](https://en.wikipedia.org/wiki/Robots_exclusion_standard)) to check whether you are \"allowed\" to scrape their website. (Here is the [robots.txt file for nytimes.com](http://www.nytimes.com/robots.txt).)\n",
    "\n",
    "### Appendix B: Web scraping resources\n",
    "\n",
    "- The [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) is written like a tutorial, and is worth reading to gain a detailed understanding of the library.\n",
    "- For more Beautiful Soup examples, see [Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/), [More web scraping with Python](http://www.gregreda.com/2013/04/29/more-web-scraping-with-python/), and this [web scraping lesson](http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html) from Stanford's \"Text As Data\" course.\n",
    "- [Web Scraping with Python](https://www.youtube.com/watch?v=p1iX0uxM1w8) is a 3-hour video tutorial covering Beautiful Soup and other scraping tools. (The [slides](https://docs.google.com/presentation/d/1uHM_esB13VuSf7O1ScGueisnrtu-6usGFD3fs4z5YCE/edit#slide=id.p) and [code](https://github.com/kjam/python-web-scraping-tutorial) are also available.)\n",
    "- [Scrapy](http://scrapy.org/) is a popular application framework that is useful for more complex web scraping projects.\n",
    "- [How a Math Genius Hacked OkCupid to Find True Love](http://www.wired.com/2014/01/how-to-hack-okcupid/all/) and [How Netflix Reverse Engineered Hollywood](http://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/?single_page=true) are two fun examples of using web scraping to build an interesting dataset.\n",
    "\n",
    "### Appendix C: Alternative syntax for Beautiful Soup\n",
    "\n",
    "It's worth noting that Beautiful Soup actually offers multiple ways to express the same command. I tend to use the most verbose option, since I think it makes the code readable, but it's useful to be able to recognize the alternative syntax since you might see it used elsewhere.\n",
    "\n",
    "For example, you can **search for a tag** by accessing it like an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
