{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbfa81c",
   "metadata": {},
   "source": [
    "## Web scraping exercise\n",
    "\n",
    "Define a generic function `SOS_help` which retrieves help results from Stack Overflow Stunning results. <br>\n",
    "\n",
    "The following command works just fine:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1b416",
   "metadata": {},
   "source": [
    "Create a function `get_SOS_help` which: <br>\n",
    "    - Prints \"works as intended\" if no error. <br>\n",
    "    - Prints the first link from stack overflow related to the error. As an example: <br>\n",
    "        `print_output(command = 'np.random.uniform(-1, 1, siz=100)'`\n",
    "        should retrieve the following link:\n",
    "        https://stackoverflow.com/questions/72537485/typeerror-uniform-got-an-unexpected-keyword-argument-low-size <br>\n",
    "    - Prints the most voted help\n",
    "    - Opens a new browser using the link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dae86a",
   "metadata": {},
   "source": [
    "## Create a malfunctioning code and use this function on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e104a0-10c1-4a6c-8e61-7ae4936c4cbe",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdfd6cb-2f10-4640-bfc5-394cc47647bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "from os import linesep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ae75a-f30a-4908-9891-1c6ed3d1ad14",
   "metadata": {},
   "source": [
    "# 2. Functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0d245-2238-4e9d-842d-88cbc2f23cc5",
   "metadata": {},
   "source": [
    "## 2.1. Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0babe4-984c-4dd0-a374-85d1f5895bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_COOKIE_CHECK = [\"h1\",\"Uo8X3b OhScic zsYMMe\"]\n",
    "STACKOVERFLOW_COOKIE_CHECK = [\"div\",\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490209b9-1783-4665-94fb-2d26e7e7a5b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2. Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2603ad66-4746-46c0-a9d2-fa4cf80a8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CookieError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621b47f-f95c-4c0c-bba9-58ff84db2a71",
   "metadata": {},
   "source": [
    "## 2.3. Pretty print functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad14671-8ab9-4556-a1fa-a503b44a0a64",
   "metadata": {},
   "source": [
    "### 2.3.1 Colors class\n",
    "\n",
    "This class has the codes to prettify python output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632f93ef-2a6e-4831-b90c-e3fa9cc69e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    ITALIC = '\\x1B[3m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b646d-bb57-4354-9969-1a4a1797b156",
   "metadata": {},
   "source": [
    "### 2.3.2 MyHTMLParser \n",
    "\n",
    "This class has the way to convert a html to a array of strings with some format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2dc7ea-e460-41cf-ac57-a755c9fb6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "    def feed(self, in_html):\n",
    "        self.output = \"\"\n",
    "        self.tag_lines = []\n",
    "        super(MyHTMLParser, self).feed(in_html)\n",
    "        return self.tag_lines\n",
    "    def handle_data(self, data):\n",
    "        if self.output not in([linesep,\"\",\" \"]):\n",
    "            self.tag_lines.append(self.output)\n",
    "        self.output = data.strip()\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        aux = \"\"\n",
    "        if tag == 'li':\n",
    "            self.output = self.output + linesep\n",
    "        elif tag == 'code':\n",
    "            pass\n",
    "        elif tag == 'blockquote' :\n",
    "            self.output  =  self.output + '\\t' \n",
    "        elif tag == 'p':\n",
    "            self.output = self.output + color.END\n",
    "        elif tag == 'div':\n",
    "            pass\n",
    "             \n",
    "    def handle_endtag(self, tag):\n",
    "\n",
    "        if tag == 'blockquote':\n",
    "            self.output += \"\"  \n",
    "        elif tag == 'code':\n",
    "            self.output = '\\t' + color.DARKCYAN + self.output + color.END\n",
    "        elif tag == 'p':\n",
    "            pass \n",
    "        elif tag == 'div':\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e2819-8ccd-40ea-8390-bf294c4df9a7",
   "metadata": {},
   "source": [
    "### 2.3.3 print_help \n",
    "\n",
    "A way to print the answers of stack overflow. the parser may not works properly so it can be switched to False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea20a07-94f8-4e03-a810-ff4d03377499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_help(content,url,parser_b = False):\n",
    "    \"\"\"\n",
    "    print the answers with little style. Code will turn cianblack if parser_b is True\n",
    "  \n",
    "    Parameters:\n",
    "    content (str): content in html to be parsed \n",
    "    url (str): direction of the web with the answers\n",
    "    parser_b (bool): if true then it will parsed with MyHTMLParser class\n",
    "  \n",
    "    Returns:\n",
    "    str: url encoded\n",
    "    \"\"\"\n",
    "    print(color.BOLD + \"FIRST STACKOVERFLOW RESPONSE IN: \"+ color.END + url ) \n",
    "    if parser_b:\n",
    "        parser = MyHTMLParser()\n",
    "        for text in parser.feed(content):\n",
    "            print(text)\n",
    "    else:\n",
    "        soup = BeautifulSoup(df['answer_html'][0], 'html.parser')\n",
    "        print(soup.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2430a55-ac70-4261-bd5e-f4d0508d32d3",
   "metadata": {},
   "source": [
    "## 2.4. Get html\n",
    "\n",
    "- `error_encode_url`\n",
    "    - error:str\n",
    "- `get_html_search`\n",
    "    - query: str\n",
    "- `check_html_mal`\n",
    "    - soup:\n",
    "- `is_valid_html`\n",
    "    - soup:bs4.BeautifulSoup, \n",
    "    - config:[str, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067e14c3-c375-4d36-88b1-03aba1c5b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_encode_url(error:str):\n",
    "    \"\"\"\n",
    "    Generate url encode from a error using a defined base in google and a web to search\n",
    "  \n",
    "    Parameters:\n",
    "    error (str): string with the error to be searched \n",
    "  \n",
    "    Returns:\n",
    "    str: url encoded\n",
    "    \"\"\"\n",
    "    base_search = \"https://www.google.com/search?q=\"\n",
    "    web = \"stackoverflow.com\"\n",
    "    search = urllib.parse.quote_plus(f'{web} {error}')\n",
    "    url_base = f'{base_search}{search}'\n",
    "    return url_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703c7633-73c7-40b0-8055-3981253e8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_search(query: str):\n",
    "    \"\"\"\n",
    "    Get the html of a web page from the a query url given.\n",
    "  \n",
    "    Parameters:\n",
    "    query (str): string with the url to be searched\n",
    "  \n",
    "    Returns:\n",
    "    str: hmlt string of the search\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36', 'Upgrade-Insecure-Requests': '1', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'DNT': '1', 'Accept-Encoding': 'gzip, deflate', 'Cookie':'CONSENT=YES+cb.20210418-17-p0.it+FX+917; '}\n",
    "    # headers = \"\"\n",
    "\n",
    "    res = requests.get(query,headers=headers)\n",
    "    if res.status_code != 200:\n",
    "        raise requests.exceptions.RequestException(\"Cannot procceed only works with 200 status code in response\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7577f40b-0690-42b3-9df1-2f0133f9d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_html_mal(soup):\n",
    "    \"\"\"\n",
    "    MALA Solo para sirve para ver el ejemlpo de como ha evolucionado:\n",
    "    me comlique mucho\n",
    "    \"\"\"\n",
    "    ret = True\n",
    "    common_button = [\"Accept all\", \"Reject all\",\"Rechazar todo\",\"Aceptar todo\"]\n",
    "    common_title = ['Antes de ir a Google', 'Before you continue to Google']\n",
    "    title_res = soup.find_all(\"h1\")\n",
    "    title_str = title_res[0].contents[0]\n",
    "    button_res = soup.find_all(\"input\",attrs={'class':'basebutton'})\n",
    "\n",
    "    for b in button_res:\n",
    "        if b.get_attribute_list(\"value\")[0] in common_button:\n",
    "            ret = False\n",
    "    if title_str in common_title:\n",
    "            ret = False\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0df90b6-d6a7-42b5-b594-a91ac6d6ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_valid_html(soup:bs4.BeautifulSoup, config:[str, str]):\n",
    "    \"\"\"\n",
    "    Check if the page url has the search result. \n",
    "    if not return False. This may happends when you have to aggre cookies policy\n",
    "    Parameters:\n",
    "    soup (bs4.BeautifulSoup): soup with the html parsed \n",
    "  \n",
    "    Returns:\n",
    "    bool: True if it is ok False if not\n",
    "    \"\"\"\n",
    "    ret = False\n",
    "    search_result = soup.find_all(config[0],attrs={'class':config[1]})\n",
    "    if len(search_result)>0:\n",
    "        ret = True\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b5ff6-27f4-4f8a-9948-eb303616a19c",
   "metadata": {},
   "source": [
    "## 2.5. Parse HTML\n",
    "\n",
    "- `parse_html`: \n",
    "    - html: str, \n",
    "    - config: [str, str]\n",
    "- `get_search_urls`: \n",
    "    - soup:bs4.BeautifulSoup\n",
    "- `get_user`: \n",
    "    - answer:bs4.element.Tag, \n",
    "    - detail: str\n",
    "- `get_answers`\n",
    "    - url:str\n",
    "- `most_rated_answer`\n",
    "    - df:pd.DataFrame\n",
    "    - sort_by:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d58d673-fb0a-4ddc-809d-ae8fd8e6d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html: str,config: [str, str]):\n",
    "    '''\n",
    "    It takes a html string and a config list as input and returns a BeautifulSoup object\n",
    "    \n",
    "    Parameters:\n",
    "    html : str\n",
    "        the html of the page\n",
    "    config : [str, str]\n",
    "        [str, str]\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each dictionary contains the following keys:\n",
    "        - question: The question\n",
    "        - answer: The answer\n",
    "        - answer_type: The type of answer (text, image, etc)\n",
    "        - image_url: The url of the image if the answer_type is image\n",
    "        - image_alt: The alt of the image if the answer_\n",
    "    \n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    if not(is_valid_html(soup,config)):\n",
    "        raise CookieError(\"Check the headers of the request, it seems google has update some required fields or its values\")\n",
    "    return soup\n",
    "\n",
    "def get_search_urls(soup:bs4.BeautifulSoup): \n",
    "    '''\n",
    "    It takes a BeautifulSoup object and returns a list of urls that are the search results\n",
    "    \n",
    "    Parameters:\n",
    "    soup : bs4.BeautifulSoup\n",
    "        the soup object of the page\n",
    "    \n",
    "    Returns:\n",
    "        A list of urls\n",
    "    \n",
    "    '''\n",
    "    #find all div that have MjjYud == search results class in google search \n",
    "    results = soup.find_all('div', attrs={'class':'MjjYud'})\n",
    "\n",
    "    urls = []\n",
    "    for result in results:\n",
    "        #find the first a that contains the href with the url of the solution\n",
    "        res = result.find(\"a\")\n",
    "        #Check if the url is in the stackoverflow site\n",
    "        if \"https://stackoverflow.com\" in res.attrs[\"href\"]:\n",
    "            urls.append(res.attrs[\"href\"])\n",
    "        \n",
    "    return urls\n",
    "\n",
    "def get_user(answer:bs4.element.Tag, detail:str):\n",
    "    '''\n",
    "    I have some problems with the user. Some times appears a blanck user, and i find some post withou itemprop (actually i cannot find those now but in case...)\n",
    "    It looks for the user's name or rank in the answer, and returns it\n",
    "    \n",
    "    Parameters\n",
    "    answer : bs4.element.Tag\n",
    "        the answer to be parsed\n",
    "    detail\n",
    "        the detail you want to extract from the user.\n",
    "    \n",
    "    Returns\n",
    "        The name of the user who posted the answer.\n",
    "    \n",
    "    '''\n",
    "    if detail == \"name\":\n",
    "        search = 'a'\n",
    "        attrs = {}\n",
    "    elif detail == \"rank\":\n",
    "        search = 'span'\n",
    "        attrs = {'class':'reputation-score'}\n",
    "    user = []\n",
    "    findings = answer.find_all('div', attrs={'class':'user-details','itemprop':'author'})\n",
    "    for pos in findings:\n",
    "        aux = pos.find(search, attrs=attrs)\n",
    "        if aux!=-1:\n",
    "            user.append(aux.contents[0])\n",
    "    if len(user) == 0:\n",
    "        findings = answer.find_all('div', attrs={'class':'user-details'})\n",
    "        for pos in findings:\n",
    "            aux = pos.find(search,attrs=attrs)\n",
    "            if aux!=-1:\n",
    "                user.append(aux.contents[0])\n",
    "    return user[0]\n",
    "\n",
    "def get_answers(url:str):\n",
    "    '''\n",
    "    It takes a url, gets the html, parses it, finds all the answers, and returns a dataframe with the\n",
    "    answers\n",
    "    \n",
    "    Parameters:\n",
    "    url : str\n",
    "        The url of the question you want to get the answers for.\n",
    "    \n",
    "    Returns:\n",
    "        A dataframe with the following columns:\n",
    "        votes: number of votes for the answer\n",
    "        time: time of the answer\n",
    "        user_name: name of the user who answered\n",
    "        user_rank: rank of the user who answered\n",
    "        answer_html: html of the answer\n",
    "    \n",
    "    '''\n",
    "    res = get_html_search(url)\n",
    "    soup_r = parse_html(res.text,STACKOVERFLOW_COOKIE_CHECK)\n",
    "    answers = soup_r.find_all('div', attrs={'class':'answer'})\n",
    "    df = pd.DataFrame({},columns = [\"votes\",\"time\",\"answer_html\",\"user_name\",\"user_rank\"])\n",
    "\n",
    "    for answer in answers:\n",
    "        votes = answer.find('div', attrs={'class':'js-vote-count'})\n",
    "        exp = answer.find('div', attrs={'class':'s-prose'})\n",
    "        time = answer.find('div', attrs={'class':'user-action-time'}).find('span', attrs={'class':'relativetime'}).attrs[\"title\"]\n",
    "        usr_name = get_user(answers[0],\"name\")\n",
    "        usr_rank = get_user(answers[0],\"rank\")\n",
    "        ans = {'votes': [int(votes.attrs[\"data-value\"])],\n",
    "               'time': [time],\n",
    "               'user_name':[usr_name],\n",
    "               'user_rank':[usr_rank],\n",
    "               'answer_html': [exp.prettify()]\n",
    "              }\n",
    "        df = pd.concat([df, pd.DataFrame(ans)])\n",
    "    return df.reset_index(drop = True)\n",
    "\n",
    "def most_rated_answer(df:pd.DataFrame,sort_by:str):\n",
    "    '''The function takes in a dataframe and a string as input and returns the most rated answer in the\n",
    "    dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        The dataframe that contains the answers\n",
    "    sort_by : str\n",
    "        \"ans\" or \"ques\"\n",
    "    \n",
    "    Returns:\n",
    "        A dataframe with the most rated answer\n",
    "    \n",
    "    '''\n",
    "    if sort_by == \"ans\":\n",
    "        df = df.sort_values(\"votes\",ascending=False).reset_index(drop = True)\n",
    "    \n",
    "    return df.iloc[[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137305b4-8336-4a23-92a6-ce3d5c6b7e84",
   "metadata": {},
   "source": [
    "## 2.6. Open URL\n",
    "- `open_url_web`: url as str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc40fa3-1389-4e21-a4c6-13197411b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url_web(url:str):\n",
    "    \"\"\"\n",
    "    Open an url in a chrome tab. \n",
    "    Requirements are the selenium module and the web driver \n",
    "    the options are to keeo alive the browser\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): url of the web to be open \n",
    "    \"\"\"\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        driver.service.stop()\n",
    "    except Exception as ex:\n",
    "        print(\"Consider to install selenium or replace the way of getting the dricer as: driver = webdriver.Chrome(ChromeDriverManager().install()))\")\n",
    "        raise(ex)\n",
    "# get Stack Overflow Stunning help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6bdbd-99ec-4de1-be8c-37012197ba2a",
   "metadata": {},
   "source": [
    "## 2.7. Get Help\n",
    "\n",
    "- `get_SOS_help`: command parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037ac084-befa-4815-9c86-dda33bca5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SOS_help(command):\n",
    "    \n",
    "    try:\n",
    "        exec(command)\n",
    "        print(\"Works as intended\")\n",
    "    except Exception as ex:\n",
    "        error_str = f'{type(ex).__name__}: {ex}'\n",
    "        r = get_html_search(error_encode_url(error_str))\n",
    "        soup = parse_html(r.text,GOOGLE_COOKIE_CHECK)\n",
    "        urls = get_search_urls(soup)\n",
    "        df = get_answers(urls[0])\n",
    "\n",
    "        ans = most_rated_answer(df,\"ans\")\n",
    "        print_help(ans['answer_html'][0],urls[0],True)\n",
    "        open_url_web(urls[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d9b9b8-90d0-43a4-a9d6-de80da1bafe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFIRST STACKOVERFLOW RESPONSE IN: \u001b[0mhttps://stackoverflow.com/questions/31721996/is-pandas-not-importing-nameerror-global-name-pandas-is-not-defined\n",
      "\u001b[0m\n",
      "You have imported it as\n",
      "\t\u001b[36mimport pandas as pd\u001b[0m\n",
      "\u001b[0m\n",
      "and calling\n",
      "\t\u001b[36m#pandas\n",
      "df = pandas.DataFrame(columns = ['Date','Unix','Ticker','DE Ratio'])\u001b[0m\n",
      "\u001b[0m\n",
      "You could either change\n",
      "\t\u001b[36mimport pandas as pd\u001b[0m\n",
      "to\n",
      "\t\u001b[36mimport pandas\u001b[0m\n",
      "or\n",
      "\t\u001b[36mdf = pandas.DataFrame(columns = ['Date','Unix','Ticker','DE Ratio'])\u001b[0m\n",
      "to\n",
      "\t\u001b[36mdf = pd.DataFrame(columns = ['Date','Unix','Ticker','DE Ratio'])\u001b[0m\n",
      ".\n",
      "\u001b[0m\n",
      "edit:\n",
      "\u001b[0m\n",
      "error is due to missing\n",
      "\t\u001b[36m)\u001b[0m\n",
      "\t\u001b[36msave = gather.replace(' ','').replace(')','').replace('(','').replace('/',''+('.csv'))\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "command = 'pandas.random.uniform(-1, 1, siz=100)'\n",
    "get_SOS_help(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7e9e6-d93f-46a1-ab72-c20c2922c515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
